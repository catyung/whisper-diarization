{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YgRnzOoPoJNa",
        "Q_NffY52oMbw",
        "fvFi6oYHxqgF",
        "18N17Jfm0wbs",
        "QKUc-dtr03_3"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6VVEv8phm6A"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# !pip install git+https://github.com/openai/whisper.git\n",
        "# !pip install setuptools-rust\n",
        "# !pip install datasets\n",
        "\n",
        "# !git clone https://github.com/catyung/whisper-diarization\n",
        "# !pip install -r requirements.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = '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'\n",
        "huggingface_hub.login(HUGGINGFACEHUB_API_TOKEN)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxaaR0dDMRkq",
        "outputId": "c3735143-9660-4628-f70b-8a2c5760dfb2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#github whisper-diarization (using .py)"
      ],
      "metadata": {
        "id": "-FaUkgfJ_BJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python diarize.py -a spanish_conversation.mp3 --whisper-model large-v2"
      ],
      "metadata": {
        "id": "5I9BSWzC_EK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# directly copy diarize.py"
      ],
      "metadata": {
        "id": "j9JCXedeJFPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### helpers.py"
      ],
      "metadata": {
        "id": "DbYEJP4KKeYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wget\n",
        "from omegaconf import OmegaConf\n",
        "import json\n",
        "import shutil\n",
        "import platform\n",
        "\n",
        "punct_model_langs = [\n",
        "    \"en\",\n",
        "    \"fr\",\n",
        "    \"de\",\n",
        "    \"es\",\n",
        "    \"it\",\n",
        "    \"nl\",\n",
        "    \"pt\",\n",
        "    \"bg\",\n",
        "    \"pl\",\n",
        "    \"cs\",\n",
        "    \"sk\",\n",
        "    \"sl\",\n",
        "]\n",
        "wav2vec2_langs = [\n",
        "    \"en\",\n",
        "    \"fr\",\n",
        "    \"de\",\n",
        "    \"es\",\n",
        "    \"it\",\n",
        "    \"nl\",\n",
        "    \"pt\",\n",
        "    \"ja\",\n",
        "    \"zh\",\n",
        "    \"uk\",\n",
        "    \"pt\",\n",
        "    \"ar\",\n",
        "    \"ru\",\n",
        "    \"pl\",\n",
        "    \"hu\",\n",
        "    \"fi\",\n",
        "    \"fa\",\n",
        "    \"el\",\n",
        "    \"tr\",\n",
        "]\n",
        "\n",
        "\n",
        "def create_config(output_dir):\n",
        "    DOMAIN_TYPE = \"telephonic\"  # Can be meeting or telephonic based on domain type of the audio file\n",
        "    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
        "    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
        "    MODEL_CONFIG = os.path.join(output_dir, CONFIG_FILE_NAME)\n",
        "    if not os.path.exists(MODEL_CONFIG):\n",
        "        MODEL_CONFIG = wget.download(CONFIG_URL, output_dir)\n",
        "\n",
        "    config = OmegaConf.load(MODEL_CONFIG)\n",
        "\n",
        "    data_dir = os.path.join(output_dir, \"data\")\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    meta = {\n",
        "        \"audio_filepath\": os.path.join(output_dir, \"mono_file.wav\"),\n",
        "        \"offset\": 0,\n",
        "        \"duration\": None,\n",
        "        \"label\": \"infer\",\n",
        "        \"text\": \"-\",\n",
        "        \"rttm_filepath\": None,\n",
        "        \"uem_filepath\": None,\n",
        "    }\n",
        "    with open(os.path.join(data_dir, \"input_manifest.json\"), \"w\") as fp:\n",
        "        json.dump(meta, fp)\n",
        "        fp.write(\"\\n\")\n",
        "\n",
        "    pretrained_vad = \"vad_multilingual_marblenet\"\n",
        "    pretrained_speaker_model = \"titanet_large\"\n",
        "\n",
        "    # num_workers = 1 results in \"pickle\" errors from Nvidia's NeMo on Silicon M chips\n",
        "    if (platform.machine() == \"arm64\") or (platform.machine() == \"aarch64\"):\n",
        "        config.num_workers = 0\n",
        "    else:\n",
        "        config.num_workers = 1  # Workaround for multiprocessing hanging with ipython issue\n",
        "\n",
        "    config.diarizer.manifest_filepath = os.path.join(data_dir, \"input_manifest.json\")\n",
        "    config.diarizer.out_dir = (\n",
        "        output_dir  # Directory to store intermediate files and prediction outputs\n",
        "    )\n",
        "\n",
        "    config.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n",
        "    config.diarizer.oracle_vad = (\n",
        "        False  # compute VAD provided with model_path to vad config\n",
        "    )\n",
        "    config.diarizer.clustering.parameters.oracle_num_speakers = False\n",
        "\n",
        "    # Here, we use our in-house pretrained NeMo VAD model\n",
        "    config.diarizer.vad.model_path = pretrained_vad\n",
        "    config.diarizer.vad.parameters.onset = 0.8\n",
        "    config.diarizer.vad.parameters.offset = 0.6\n",
        "    config.diarizer.vad.parameters.pad_offset = -0.05\n",
        "    config.diarizer.msdd_model.model_path = (\n",
        "        \"diar_msdd_telephonic\"  # Telephonic speaker diarization model\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_word_ts_anchor(s, e, option=\"start\"):\n",
        "    if option == \"end\":\n",
        "        return e\n",
        "    elif option == \"mid\":\n",
        "        return (s + e) / 2\n",
        "    return s\n",
        "\n",
        "\n",
        "def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option=\"start\"):\n",
        "    s, e, sp = spk_ts[0]\n",
        "    wrd_pos, turn_idx = 0, 0\n",
        "    wrd_spk_mapping = []\n",
        "\n",
        "    last_end = 0\n",
        "    for wrd_dict in wrd_ts:\n",
        "        if 'start' in wrd_dict.keys():\n",
        "            ws, we, wrd = (\n",
        "                int(wrd_dict[\"start\"] * 1000),\n",
        "                int(wrd_dict[\"end\"] * 1000),\n",
        "                wrd_dict['word'],\n",
        "            )\n",
        "            last_end = we\n",
        "        else:\n",
        "            ws = last_end + 1\n",
        "            we = ws + 1\n",
        "            wrd = wrd_dict['word']\n",
        "            last_end = we\n",
        "        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n",
        "        while wrd_pos > float(e):\n",
        "            turn_idx += 1\n",
        "            turn_idx = min(turn_idx, len(spk_ts) - 1)\n",
        "            s, e, sp = spk_ts[turn_idx]\n",
        "            if turn_idx == len(spk_ts) - 1:\n",
        "                e = get_word_ts_anchor(ws, we, option=\"end\")\n",
        "        wrd_spk_mapping.append(\n",
        "            {\"word\": wrd, \"start_time\": ws, \"end_time\": we, \"speaker\": sp}\n",
        "        )\n",
        "    return wrd_spk_mapping\n",
        "\n",
        "\n",
        "sentence_ending_punctuations = \".?!\"\n",
        "\n",
        "\n",
        "def get_first_word_idx_of_sentence(word_idx, word_list, speaker_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    left_idx = word_idx\n",
        "    while (\n",
        "        left_idx > 0\n",
        "        and word_idx - left_idx < max_words\n",
        "        and speaker_list[left_idx - 1] == speaker_list[left_idx]\n",
        "        and not is_word_sentence_end(left_idx - 1)\n",
        "    ):\n",
        "        left_idx -= 1\n",
        "\n",
        "    return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
        "\n",
        "\n",
        "def get_last_word_idx_of_sentence(word_idx, word_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    right_idx = word_idx\n",
        "    while (\n",
        "        right_idx < len(word_list)\n",
        "        and right_idx - word_idx < max_words\n",
        "        and not is_word_sentence_end(right_idx)\n",
        "    ):\n",
        "        right_idx += 1\n",
        "\n",
        "    return (\n",
        "        right_idx\n",
        "        if right_idx == len(word_list) - 1 or is_word_sentence_end(right_idx)\n",
        "        else -1\n",
        "    )\n",
        "\n",
        "\n",
        "def get_realigned_ws_mapping_with_punctuation(\n",
        "    word_speaker_mapping, max_words_in_sentence=50\n",
        "):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0\n",
        "        and word_speaker_mapping[x][\"word\"][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    wsp_len = len(word_speaker_mapping)\n",
        "\n",
        "    words_list, speaker_list = [], []\n",
        "    for k, line_dict in enumerate(word_speaker_mapping):\n",
        "        word, speaker = line_dict[\"word\"], line_dict[\"speaker\"]\n",
        "        words_list.append(word)\n",
        "        speaker_list.append(speaker)\n",
        "\n",
        "    k = 0\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k]\n",
        "        if (\n",
        "            k < wsp_len - 1\n",
        "            and speaker_list[k] != speaker_list[k + 1]\n",
        "            and not is_word_sentence_end(k)\n",
        "        ):\n",
        "            left_idx = get_first_word_idx_of_sentence(\n",
        "                k, words_list, speaker_list, max_words_in_sentence\n",
        "            )\n",
        "            right_idx = (\n",
        "                get_last_word_idx_of_sentence(\n",
        "                    k, words_list, max_words_in_sentence - k + left_idx - 1\n",
        "                )\n",
        "                if left_idx > -1\n",
        "                else -1\n",
        "            )\n",
        "            if min(left_idx, right_idx) == -1:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            spk_labels = speaker_list[left_idx : right_idx + 1]\n",
        "            mod_speaker = max(set(spk_labels), key=spk_labels.count)\n",
        "            if spk_labels.count(mod_speaker) < len(spk_labels) // 2:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            speaker_list[left_idx : right_idx + 1] = [mod_speaker] * (\n",
        "                right_idx - left_idx + 1\n",
        "            )\n",
        "            k = right_idx\n",
        "\n",
        "        k += 1\n",
        "\n",
        "    k, realigned_list = 0, []\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k].copy()\n",
        "        line_dict[\"speaker\"] = speaker_list[k]\n",
        "        realigned_list.append(line_dict)\n",
        "        k += 1\n",
        "\n",
        "    return realigned_list\n",
        "\n",
        "\n",
        "def get_sentences_speaker_mapping(word_speaker_mapping, spk_ts):\n",
        "    s, e, spk = spk_ts[0]\n",
        "    prev_spk = spk\n",
        "\n",
        "    snts = []\n",
        "    snt = {\"speaker\": f\"Speaker {spk}\", \"start_time\": s, \"end_time\": e, \"text\": \"\"}\n",
        "\n",
        "    for wrd_dict in word_speaker_mapping:\n",
        "        wrd, spk = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
        "        s, e = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
        "        if spk != prev_spk:\n",
        "            snts.append(snt)\n",
        "            snt = {\n",
        "                \"speaker\": f\"Speaker {spk}\",\n",
        "                \"start_time\": s,\n",
        "                \"end_time\": e,\n",
        "                \"text\": \"\",\n",
        "            }\n",
        "        else:\n",
        "            snt[\"end_time\"] = e\n",
        "        snt[\"text\"] += wrd + \" \"\n",
        "        prev_spk = spk\n",
        "\n",
        "    snts.append(snt)\n",
        "    return snts\n",
        "\n",
        "\n",
        "def get_speaker_aware_transcript(sentences_speaker_mapping, f):\n",
        "    for sentence_dict in sentences_speaker_mapping:\n",
        "        sp = sentence_dict[\"speaker\"]\n",
        "        text = sentence_dict[\"text\"]\n",
        "        f.write(f\"\\n\\n{sp}: {text}\")\n",
        "\n",
        "\n",
        "def format_timestamp(\n",
        "    milliseconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n",
        "):\n",
        "    assert milliseconds >= 0, \"non-negative timestamp expected\"\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n",
        "    return (\n",
        "        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def write_srt(transcript, file):\n",
        "    \"\"\"\n",
        "    Write a transcript to a file in SRT format.\n",
        "\n",
        "    \"\"\"\n",
        "    for i, segment in enumerate(transcript, start=1):\n",
        "        # write srt lines\n",
        "        print(\n",
        "            f\"{i}\\n\"\n",
        "            f\"{format_timestamp(segment['start_time'], always_include_hours=True, decimal_marker=',')} --> \"\n",
        "            f\"{format_timestamp(segment['end_time'], always_include_hours=True, decimal_marker=',')}\\n\"\n",
        "            f\"{segment['speaker']}: {segment['text'].strip().replace('-->', '->')}\\n\",\n",
        "            file=file,\n",
        "            flush=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def cleanup(path: str):\n",
        "    \"\"\"path could either be relative or absolute.\"\"\"\n",
        "    # check if file or directory exists\n",
        "    if os.path.isfile(path) or os.path.islink(path):\n",
        "        # remove file\n",
        "        os.remove(path)\n",
        "    elif os.path.isdir(path):\n",
        "        # remove directory and all its content\n",
        "        shutil.rmtree(path)\n",
        "    else:\n",
        "        raise ValueError(\"Path {} is not a file or dir.\".format(path))"
      ],
      "metadata": {
        "id": "L6MIcjpFKsbW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### above is helpers.py (with only text -> words fix)"
      ],
      "metadata": {
        "id": "-zSQpAFnKkvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = 'french_90min.mp3'\n",
        "model_name = 'large-v2'\n",
        "stemming = False"
      ],
      "metadata": {
        "id": "GRkpdcpBJfD3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### fake parser, take the above input"
      ],
      "metadata": {
        "id": "bN4KDd_-NQOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import argparse\n",
        "import os\n",
        "# from helpers import *\n",
        "from faster_whisper import WhisperModel\n",
        "import whisperx\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile\n",
        "#from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "import re\n",
        "import logging\n",
        "import subprocess\n",
        "\n",
        "mtypes = {'cpu': 'int8', 'cuda': 'float16'}\n",
        "\n",
        "# Initialize parser\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"-a\", \"--audio\", help=\"name of the target audio file\", required=True\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--no-stem\",\n",
        "    action=\"store_false\",\n",
        "    dest=\"stemming\",\n",
        "    default=True,\n",
        "    help=\"Disables source separation.\"\n",
        "    \"This helps with long files that don't contain a lot of music.\",\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--whisper-model\",\n",
        "    dest=\"model_name\",\n",
        "    default=\"medium.en\",\n",
        "    help=\"name of the Whisper model to use\",\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--device\",\n",
        "    dest=\"device\",\n",
        "    default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    help=\"if you have a GPU use 'cuda', otherwise 'cpu'\",\n",
        ")\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    audio=audio_path,\n",
        "    stemming=stemming,\n",
        "    model_name=model_name,\n",
        "    device='cuda'\n",
        ")\n",
        "\n",
        "if args.stemming:\n",
        "    # Isolate vocals from the rest of the audio\n",
        "\n",
        "    return_code = os.system(\n",
        "        f'python3 -m demucs.separate -n htdemucs --two-stems=vocals \"{args.audio}\" -o \"temp_outputs\"'\n",
        "    )\n",
        "\n",
        "    if return_code != 0:\n",
        "        logging.warning(\n",
        "            \"Source splitting failed, using original audio file. Use --no-stem argument to disable it.\"\n",
        "        )\n",
        "        vocal_target = args.audio\n",
        "    else:\n",
        "        vocal_target = os.path.join(\n",
        "            \"temp_outputs\", \"htdemucs\", os.path.basename(args.audio[:-4]), \"vocals.wav\"\n",
        "        )\n",
        "else:\n",
        "    vocal_target = args.audio\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhmaHhnxJLN7",
        "outputId": "31f161a1-da49-44fa-c253-0f5082591816"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.2 s, sys: 162 ms, total: 1.36 s\n",
            "Wall time: 3min 53s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### load model (skip if loaded alrdy)"
      ],
      "metadata": {
        "id": "ivvX0_LDNUau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Run on GPU with FP16\n",
        "whisper_model = WhisperModel(\n",
        "    args.model_name, device=args.device, compute_type=mtypes[args.device])\n",
        "\n",
        "# or run on GPU with INT8\n",
        "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
        "# or run on CPU with INT8\n",
        "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8bTRdc_LaHV",
        "outputId": "a4935f32-4a18-4dc5-ed34-86d33c2a70f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.18 s, sys: 3.17 s, total: 4.34 s\n",
            "Wall time: 30.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ㅤ"
      ],
      "metadata": {
        "id": "PhwE40InNf52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nemo_process = subprocess.run(\n",
        "    [\"python3\", \"nemo_process.py\", \"-a\", vocal_target, \"--device\", args.device],\n",
        "    )\n",
        "\n",
        "segments, info = whisper_model.transcribe(\n",
        "    vocal_target, beam_size=1, word_timestamps=True\n",
        ")\n",
        "whisper_results = []\n",
        "for segment in segments:\n",
        "    whisper_results.append(segment._asdict())\n",
        "# clear gpu vram\n",
        "# del whisper_model\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "if info.language in wav2vec2_langs:\n",
        "    alignment_model, metadata = whisperx.load_align_model(\n",
        "        language_code=info.language, device=args.device\n",
        "    )\n",
        "    result_aligned = whisperx.align(\n",
        "        whisper_results, alignment_model, metadata, vocal_target, args.device\n",
        "    )\n",
        "    word_timestamps = result_aligned[\"word_segments\"]\n",
        "    # clear gpu vram\n",
        "    # del alignment_model\n",
        "    # torch.cuda.empty_cache()\n",
        "else:\n",
        "    word_timestamps = []\n",
        "    for segment in whisper_results:\n",
        "        for word in segment[\"words\"]:\n",
        "            word_timestamps.append({\"text\": word[2], \"start\": word[0], \"end\": word[1]})\n",
        "\n",
        "\n",
        "# convert audio to mono for NeMo combatibility\n",
        "signal, sample_rate = librosa.load(vocal_target, sr=None)\n",
        "ROOT = os.getcwd()\n",
        "temp_path = os.path.join(ROOT, \"temp_outputs\")\n",
        "os.makedirs(temp_path, exist_ok=True)\n",
        "soundfile.write(os.path.join(temp_path, \"mono_file.wav\"), signal, sample_rate, \"PCM_24\")\n",
        "\n",
        "# Initialize NeMo MSDD diarization model\n",
        "#msdd_model = NeuralDiarizer(cfg=create_config(temp_path)).to(args.device)\n",
        "#msdd_model.diarize()\n",
        "\n",
        "#del msdd_model\n",
        "#torch.cuda.empty_cache()\n",
        "\n",
        "# Reading timestamps <> Speaker Labels mapping\n",
        "\n",
        "\n",
        "speaker_ts = []\n",
        "with open(os.path.join(temp_path, \"pred_rttms\", \"mono_file.rttm\"), \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line_list = line.split(\" \")\n",
        "        s = int(float(line_list[5]) * 1000)\n",
        "        e = s + int(float(line_list[8]) * 1000)\n",
        "        speaker_ts.append([s, e, int(line_list[11].split(\"_\")[-1])])\n",
        "\n",
        "wsm = get_words_speaker_mapping(word_timestamps, speaker_ts, \"start\")\n",
        "\n",
        "if info.language in punct_model_langs:\n",
        "    # restoring punctuation in the transcript to help realign the sentences\n",
        "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
        "\n",
        "    words_list = list(map(lambda x: x[\"word\"], wsm))\n",
        "\n",
        "    labled_words = punct_model.predict(words_list)\n",
        "\n",
        "    ending_puncts = \".?!\"\n",
        "    model_puncts = \".,;:!?\"\n",
        "\n",
        "    # We don't want to punctuate U.S.A. with a period. Right?\n",
        "    is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
        "\n",
        "    for word_dict, labeled_tuple in zip(wsm, labled_words):\n",
        "        word = word_dict[\"word\"]\n",
        "        if (\n",
        "            word\n",
        "            and labeled_tuple[1] in ending_puncts\n",
        "            and (word[-1] not in model_puncts or is_acronym(word))\n",
        "        ):\n",
        "            word += labeled_tuple[1]\n",
        "            if word.endswith(\"..\"):\n",
        "                word = word.rstrip(\".\")\n",
        "            word_dict[\"word\"] = word\n",
        "\n",
        "    wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
        "else:\n",
        "    logging.warning(\n",
        "        f'Punctuation restoration is not available for {info.language} language.'\n",
        "    )\n",
        "\n",
        "ssm = get_sentences_speaker_mapping(wsm, speaker_ts)\n",
        "\n",
        "with open(f\"{args.audio[:-4]}.txt\", \"w\", encoding=\"utf-8-sig\") as f:\n",
        "    get_speaker_aware_transcript(ssm, f)\n",
        "\n",
        "with open(f\"{args.audio[:-4]}.srt\", \"w\", encoding=\"utf-8-sig\") as srt:\n",
        "    write_srt(ssm, srt)\n",
        "\n",
        "# cleanup(temp_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7U4LSVJNdtE",
        "outputId": "b51cce57-e0f6-43d2-f551-f820e5c1e5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_voxpopuli_base_10k_asr_fr.pt\" to /root/.cache/torch/hub/checkpoints/wav2vec2_voxpopuli_base_10k_asr_fr.pt\n",
            "100%|██████████| 360M/360M [00:06<00:00, 56.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "def cleanup(path: str):\n",
        "    \"\"\"path could either be relative or absolute.\"\"\"\n",
        "    # check if file or directory exists\n",
        "    if os.path.isfile(path) or os.path.islink(path):\n",
        "        # remove file\n",
        "        os.remove(path)\n",
        "    elif os.path.isdir(path):\n",
        "        # remove directory and all its content\n",
        "        shutil.rmtree(path)\n",
        "    else:\n",
        "        raise ValueError(\"Path {} is not a file or dir.\".format(path))\n",
        "\n",
        "cleanup('./temp_outputs')\n"
      ],
      "metadata": {
        "id": "TC-raED2PVGf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5sJEQ3vPYMO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}